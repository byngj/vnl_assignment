{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf9e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataset\n",
    "from dataset.datasets import C100Dataset\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b5e8d",
   "metadata": {},
   "source": [
    "## 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30345d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nl = C100Dataset('./dataset/data/cifar100_nl.csv')\n",
    "[data_nl_tr_x, data_nl_tr_y, _, _] = dataset_nl.getDataset()\n",
    "dataset_nl_test = C100Dataset('./dataset/data/cifar100_nl_test.csv')\n",
    "[data_nl_ts_x, data_nl_ts_y, _, _] = dataset_nl_test.getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bb023c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 3, 32, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nl_tr_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe2eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"blur a single image on CPU\"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        radias = kernel_size // 2\n",
    "        kernel_size = radias * 2 + 1\n",
    "        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),\n",
    "                                stride=1, padding=0, bias=False, groups=3)\n",
    "        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),\n",
    "                                stride=1, padding=0, bias=False, groups=3)\n",
    "        self.k = kernel_size\n",
    "        self.r = radias\n",
    "\n",
    "        self.blur = nn.Sequential(\n",
    "            nn.ReflectionPad2d(radias),\n",
    "            self.blur_h,\n",
    "            self.blur_v\n",
    "        )\n",
    "\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.pil_to_tensor(img).unsqueeze(0)\n",
    "\n",
    "        sigma = np.random.uniform(0.1, 2.0)\n",
    "        x = np.arange(-self.r, self.r + 1)\n",
    "        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n",
    "        x = x / x.sum()\n",
    "        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n",
    "\n",
    "        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n",
    "        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = self.blur(img)\n",
    "            img = img.squeeze()\n",
    "\n",
    "        img = self.tensor_to_pil(img)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebf844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    GaussianBlur(kernel_size=int(0.1 * 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f821db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_dataset(N, batch_size, x, y):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(N//batch_size):\n",
    "        image_batch = []\n",
    "        label_batch = []\n",
    "        for j in range(batch_size):\n",
    "            image_batch.append(x[i * batch_size + j])\n",
    "            label_batch.append(y[i * batch_size + j])\n",
    "        images.append(image_batch)\n",
    "        labels.append(label_batch)\n",
    "        \n",
    "    images = np.array(images).reshape(N//batch_size, batch_size, 3, 32, 32)\n",
    "    labels = np.array(labels).reshape(N//batch_size, batch_size)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26084e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_batch_dataset(N=data_nl_tr_y.shape[0], batch_size=128, x=data_nl_tr_x, y = data_nl_tr_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52d3cf",
   "metadata": {},
   "source": [
    "## 2. Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a794dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(ResNet_, self).__init__()\n",
    "        self.resnet_dict = {\"resnet18\": models.resnet18(pretrained=False, num_classes=out_dim),\n",
    "                            \"resnet50\": models.resnet50(pretrained=False, num_classes=out_dim)}\n",
    "\n",
    "        self.backbone = self.get_basemodel(base_model)\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "\n",
    "        # add mlp projection head\n",
    "        self.backbone.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.backbone.fc)\n",
    "\n",
    "    def get_basemodel(self, model_name):\n",
    "        model = self.resnet_dict[model_name]\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebbbd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet_(base_model='resnet18', out_dim=128)\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf380fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(data_loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            pred_list.append(outputs)\n",
    "            label_list.append(labels)\n",
    "            \n",
    "    return pred_list, label_list\n",
    "\n",
    "def _accuracy(output, target, topk=(1,)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            k_score = (correct_k.mul_(100.0 / batch_size))\n",
    "            res.append(k_score.item())\n",
    "        return res\n",
    "    \n",
    "def _accuracy_sep(output, target, topk=(1,)):\n",
    "    num_class = torch.max(target)+1\n",
    "    mask_head = target < num_class//2\n",
    "    mask_tail = target >= num_class//2\n",
    "    \n",
    "    res_head = _accuracy(output[mask_head], target[mask_head])\n",
    "    res_tail = _accuracy(output[mask_tail], target[mask_tail])\n",
    "    return res_head, res_tail\n",
    "    \n",
    "def compute_accuracy(data_loader, model, topk=(1,)):\n",
    "    pred_list, label_list = _predict(data_loader, model)\n",
    "    pred_list = torch.cat(pred_list, dim=0)\n",
    "    label_list = torch.cat(label_list, dim=0)\n",
    "    res = _accuracy(pred_list, label_list, topk)\n",
    "    res_head, res_tail = _accuracy_sep(pred_list, label_list, topk)\n",
    "    return res, res_head, res_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f53c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m# training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, criterion, optimizer, scheduler, EPOCHS)\u001b[0m\n\u001b[1;32m      7\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m image_batch\n\u001b[1;32m      8\u001b[0m label_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_batch)\n\u001b[0;32m----> 9\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m \u001b[43mimage_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(image_batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_dataset, criterion, optimizer, scheduler, EPOCHS):\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "        for image_batch, label_batch in zip(*train_dataset):\n",
    "            image_batch = torch.tensor(image_batch)\n",
    "            image_batch = image_batch\n",
    "            label_batch = torch.tensor(label_batch)\n",
    "            image_batch = image_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            \n",
    "            pred = model(image_batch)\n",
    "            loss = criterion(pred, label_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(np.mean(loss_history))\n",
    "        scheduler.step()\n",
    "    \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128 \n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 200\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "    \n",
    "print('# training')\n",
    "train(model, train_dataset, criterion, optimizer, scheduler, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d4b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
